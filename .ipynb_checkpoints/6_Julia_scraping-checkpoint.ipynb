{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Julia scraps too!\n",
    "\n",
    "## A worked through example\n",
    "\n",
    "Scraping with Julia is very similar to scraping with R (or to scraping with any other language).\n",
    "\n",
    "An advantage is the broadcasting and mapping are built in in Julia through the dot `.` operator and the `map()` functions. (If this is completely new to you, it's time to go look it up in the previous labs or looking through julia documentations.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The packages we introduce this time are: `Cascadia` for searching and dealing with the html pages, `Gumbo` for transforming the downloaded page into something Julia and Cascadia can deal with, and `HTTP` to handle internet connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using Pkg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Pkg.add(\"HTTP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Pkg.add(\"Gumbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Pkg.add(\"Cascadia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using HTTP, Gumbo, Cascadia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is getting a page. This is done over an \"HTTP get request\" (refer to the R part of this lab to learn more about API queries.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stallman_page = HTTP.get(\"https://stallman.org/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the page, we get the part we are interested in: the body of the response. And we parse it, using Gumbo, to something we can deal with (instead of Gumbo native format). Notice that we go through a `String` conversion before parsing. Try and think why we need to do it (it has to do with the type of the objects we create)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_page = stallman_page.body |>\n",
    "  String |>\n",
    "  parsehtml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Than, as for the R part, we use the browser to identify what we look for. In this case I want to extract all the link where Stallman speaks about \"What's bad about X\". They are in a css class called \"c2\" and the links are stored in a subclass of that named \"a\" (this is often the case for links, and I think \"a\" stands for \"anchor\"). I discovered it by looking at the source page of Stallman's website using the \"inspector mode\" in Firefox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a selector to get the thing we want\n",
    "sel_for_c2 = Selector(\".c2 a\")\n",
    "# and we extract each matching node in the XML document\n",
    "c2_part_stallman_page = eachmatch(sel_for_c2,parsed_page.root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need a little bit of care. By default, `eachmatch()` returns an array. If you read carefully the output of the previous cell, you see:\n",
    "\n",
    "> 1-element Array{Gumbo.HTMLNode,1}:  \n",
    "> Gumbo.HTMLElement{:div}:\n",
    "\n",
    "In this case it is an array of length 24, because 24 bits of html were of the right class (\"c2\") in the page. To get to any of those we can index the array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2_part_stallman_page[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if you read carefully, the result is a Gumbo.HTMLelement, and so we can use Cascadia to work on it.\n",
    "\n",
    "All this blocks of html are cointaned within a `<a> ... </a>` delimeter. The links themselved are in a \"href\" attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information is there, we are quite close! If we want to extract the \"href\" attribute from one element we can use `getattr()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_about_links = getattr(c2_part_stallman_page[1],\"href\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we may be tempted to use broadcasting (i.e., adding a dot, ., after the function we call) and apply `getattr()` to all the elements in the array. And it may even work in lucky situation! It does for us in this situation, :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "badabout_links = getattr.(c2_part_stallman_page,\"href\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But be careful, it does not work in general because you are never sure that every elements in your array contains the right information. If only one of those elements does not have a link the function would fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Julia, the functionalities of `purrr`'s `map()` are given by the base available `map`. Let's use it to read all those links. First we need to define what function to apply on each of those strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const baseURL = \"https://stallman.org\"\n",
    "# warning! I didn't include the last \"/\" because all the links have already it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first try out on 1 link:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_link = badabout_links[1] # let's focus only on the first link\n",
    "response = HTTP.request(\"GET\",\"$baseURL$first_link\") # we read the right page glueing together the base url and the link we got before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And parse it as we have done above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we get the body of the reponse, convert it into a String, and parse it as a Gumbo HTML document\n",
    "pagebad = response.body |> String |> parsehtml "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we extract all the text we can find in that page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_string = nodeText(pagebad.root) \n",
    "result_string |> println"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's transform this flow into a function, and output everything to a Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_badness(link)\n",
    "  response = HTTP.request(\"GET\",\"$baseURL$link\") # we read the right page glueing together the base url and the link we got before\n",
    "  pagebad = response.body |> String |> parsehtml\n",
    "  result_string = nodeText(pagebad.root)\n",
    "  df = DataFrame(Link = link, Badness = result_string)\n",
    "  return df\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "badabout_links[24] |> get_badness #|> println"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to work, at least for one element at a time. Let's see on two elements at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allbad = vcat(map(get_badness,badabout_links[1:2])...)\n",
    "# notice the use of ... at the end of vcat\n",
    "# if you don't know what it does, look at the man page for it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, now let's do all of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allbad = map(get_badness,badabout_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No luck: some of the stuff we are trying to scrape is not an html file as we expect but who-knows-what. We need to play a tad safer.  \n",
    "In Julia we can do this by using a `try - catch` construct: we `try` to do something, and if we get an error instead of a lucky result we `catch` it and do something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_badness(link)\n",
    "  try\n",
    "    response = HTTP.request(\"GET\",\"$baseURL$link\") # we read the right page glueing together the base url and the link we got before \n",
    "    pagebad = response.body |> String |> parsehtml\n",
    "    result_string = nodeText(pagebad.root)\n",
    "    df = DataFrame(Link = link, Badness = result_string)\n",
    "    return df \n",
    "  catch\n",
    "    result_string = \"no html content\" \n",
    "    df = DataFrame(Link = link, Badness = result_string)\n",
    "    return df\n",
    "  end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's try that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allbad = vcat(map(get_badness,badabout_links)...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn\n",
    "\n",
    "Now that we have driven through a scraping example together, try selecting another page (or something else in Stallman's page) and scrape it on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API\n",
    "\n",
    "are just the same of R. Here, the package that you will most often rely on is HTTP.jl. Yet, the logic you will employ (using a function to paste together the necessary URL and waiting for an answer from a remote server) is always the same.\n",
    "\n",
    "You can learn more about HTTP at its webpage: https://juliaweb.github.io/HTTP.jl/stable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.1",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
